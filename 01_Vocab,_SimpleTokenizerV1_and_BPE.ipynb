{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "import os\n",
        "import re\n",
        "import tiktoken\n"
      ],
      "metadata": {
        "id": "b64TJUQWgUoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SimpleTokenzierV1"
      ],
      "metadata": {
        "id": "zLpMb5c2jI0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for item in os.listdir('.'): # '.' refers to the current directory\n",
        "#     print(item)\n",
        "\n",
        "os.getcwd()"
      ],
      "metadata": {
        "id": "nVjdmc09dmcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_mlE3ClQbeA"
      },
      "outputs": [],
      "source": [
        "with open('the-verdict.txt', 'r', encoding='utf-8') as f:\n",
        "  raw_txt = f.read()\n",
        "\n",
        "print(len(raw_txt))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove whitespace or not?\n",
        "#   Removing whitespaces reduces memory and computing requirement\n",
        "#   White spaces can be useful for text sensitive to the structure, like python indention\n",
        "preprossed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_txt)\n",
        "preprossed = [t.strip() for t in preprossed if t.strip()]\n",
        "print(preprossed[:10])\n",
        "print(len(preprossed))"
      ],
      "metadata": {
        "id": "Vz6HQH1ZgQqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocabulary: all the unique tokens in alphbetically order\n",
        "\n",
        "\n",
        "# For tokens not in the vocab\n",
        "UNKNOWN_TOKEN = \"<|unk|>\"\n",
        "\n",
        "# Added between text sources.\n",
        "# Allow the LLM to process and understand the data better.\n",
        "END_OF_TEXT_TOKEN = \"<|endoftext|>\"\n",
        "\n",
        "# Following special tokens are used by different types of tokenizers\n",
        "# [BOS]: beginning of sequence\n",
        "# [EOS]: end of sequence\n",
        "# [PAD]: padding\n",
        "\n",
        "sorted_unique_tokens = sorted(set(preprossed))\n",
        "sorted_unique_tokens.extend([END_OF_TEXT_TOKEN, UNKNOWN_TOKEN])\n",
        "print(len(sorted_unique_tokens))"
      ],
      "metadata": {
        "id": "9HV7FQ0hh1dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode token to token id\n",
        "vocab = {token:id for id,token in enumerate(sorted_unique_tokens)}\n",
        "\n",
        "# for i, item in enumerate(vocab.items()):\n",
        "#   print(i, item)\n",
        "#   if i > 20:\n",
        "#     break"
      ],
      "metadata": {
        "id": "Pnra95vEkSJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenzierV1:\n",
        "  def __init__(self, vocab):\n",
        "    self.token_to_id = vocab\n",
        "    self.id_to_token = {id:token for token,id in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [t.strip() for t in preprocessed if t.strip()]\n",
        "    preprocessed = [t if t in self.token_to_id else UNKNOWN_TOKEN for t in preprocessed]\n",
        "    return [self.token_to_id[t] for t in preprocessed]\n",
        "\n",
        "  def decode(self, ids):\n",
        "    tokens = [self.id_to_token[id] for id in ids]\n",
        "    text = \" \".join(tokens)\n",
        "    return re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)"
      ],
      "metadata": {
        "id": "FKppEdWFlMXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_v1 = SimpleTokenzierV1(vocab)"
      ],
      "metadata": {
        "id": "-4orvWMmoGt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"how are you, jessica!\"\n",
        "text2 = \"do you like tea?\"\n",
        "test_ids = tokenizer_v1.encode(\" <|endoftext|> \".join((text1, text2)))\n",
        "print(test_ids)\n",
        "print(tokenizer_v1.decode(test_ids))"
      ],
      "metadata": {
        "id": "1gOPlTcDoLJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BPE"
      ],
      "metadata": {
        "id": "Xr6Lw-Pl6-Jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_bpe = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "\n",
        "text1 = \"how are you, jessica!\"\n",
        "text2 = \"doyou like tea? å¥½ ofjweoifewjfoiewh\"\n",
        "test_ids = tokenizer_bpe.encode(\" <|endoftext|> \".join((text1, text2)), allowed_special={END_OF_TEXT_TOKEN})\n",
        "print(test_ids)\n",
        "print(tokenizer_bpe.decode(test_ids))"
      ],
      "metadata": {
        "id": "PhY5LMQn6_mJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}