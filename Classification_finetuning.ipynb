{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tiktoken\n",
        "import os\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "import tiktoken\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add \"utils/\" to sys.path\n",
        "repo_dir = os.getcwd()\n",
        "custom_files_dir = os.path.join(repo_dir, 'utils')\n",
        "sys.path.append(custom_files_dir)"
      ],
      "metadata": {
        "id": "rDNmv2nO1z1R"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "4pNDm5i1Jfwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloaded from \"SMS Spam Collection\" UC Irvine\n",
        "spam_df = pd.read_csv('Classification_finetuning/spam.csv', sep=',', header=None, names=['Label', 'Text', 'a', 'b', 'c'], encoding_errors='ignore')\n",
        "spam_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "a8RtD9s8_9nr",
        "outputId": "fa8c6957-b4a2-4268-b82e-c70b68d562fc"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Label                                               Text    a    b    c\n",
              "0      ham  Go until jurong point, crazy.. Available only ...  NaN  NaN  NaN\n",
              "1      ham                      Ok lar... Joking wif u oni...  NaN  NaN  NaN\n",
              "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...  NaN  NaN  NaN\n",
              "3      ham  U dun say so early hor... U c already then say...  NaN  NaN  NaN\n",
              "4      ham  Nah I don't think he goes to usf, he lives aro...  NaN  NaN  NaN\n",
              "...    ...                                                ...  ...  ...  ...\n",
              "5567  spam  This is the 2nd time we have tried 2 contact u...  NaN  NaN  NaN\n",
              "5568   ham               Will _ b going to esplanade fr home?  NaN  NaN  NaN\n",
              "5569   ham  Pity, * was in mood for that. So...any other s...  NaN  NaN  NaN\n",
              "5570   ham  The guy did some bitching but I acted like i'd...  NaN  NaN  NaN\n",
              "5571   ham                         Rofl. Its true to its name  NaN  NaN  NaN\n",
              "\n",
              "[5572 rows x 5 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Text</th>\n",
              "      <th>a</th>\n",
              "      <th>b</th>\n",
              "      <th>c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>spam</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>ham</td>\n",
              "      <td>Will _ b going to esplanade fr home?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>ham</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>ham</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>ham</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5572 rows × 5 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(spam_df['Label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBl8ZOgmApe6",
        "outputId": "349212d5-25e1-41cf-8d52-c2a50fa52d4d"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label\n",
            "ham     4825\n",
            "spam     747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spam_df['Label'] = spam_df['Label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# training: 70%\n",
        "# validation: 10%\n",
        "# testing: 20%\n",
        "def random_split(df, train_frac, validation_frac):\n",
        "  # Shuffle\n",
        "  df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
        "\n",
        "  train_end = int(len(df) * train_frac)\n",
        "  validation_end = train_end + int(len(df) * validation_frac)\n",
        "\n",
        "  return df[:train_end], df[train_end:validation_end], df[validation_end:]\n",
        "\n",
        "train_df, validation_df, test_df = random_split(spam_df, 0.7, 0.1)\n"
      ],
      "metadata": {
        "id": "gItBQCBKDFYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Train dataset: len={len(train_df)}')\n",
        "print(f'Validation dataset: len={len(validation_df)}')\n",
        "print(f'Test dataset: len={len(test_df)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50ylJ34TEcQ3",
        "outputId": "25bca1c2-c0f2-46ab-b22a-ac3ad5c85761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset: len=3900\n",
            "Validation dataset: len=557\n",
            "Test dataset: len=1115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the data to csv files\n",
        "train_df.to_csv('Classification_finetuning/train.csv', index=None)\n",
        "validation_df.to_csv('Classification_finetuning/validation.csv', index=None)\n",
        "test_df.to_csv('Classification_finetuning/test.csv', index=None)"
      ],
      "metadata": {
        "id": "XYgL9A9IFWCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset and Dataloader"
      ],
      "metadata": {
        "id": "xikkrAPX2BKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SpamDataset(Dataset):\n",
        "  def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
        "    self.data = pd.read_csv(csv_file)\n",
        "    self.encoded_text = [tokenizer.encode(text) for text in self.data['Text']]\n",
        "\n",
        "    if max_length is None:\n",
        "      self.max_length = self._longest_encoded_length()\n",
        "    else:\n",
        "      self.max_length = max_length\n",
        "      self.encoded_text = [\n",
        "          text[:self.max_length] for text in self.encoded_text\n",
        "      ]\n",
        "    # Pad sequences to the longest sequence\n",
        "    self.encoded_text = [\n",
        "        text + [pad_token_id] * (self.max_length - len(text))\n",
        "        for text in self.encoded_text\n",
        "    ]\n",
        "\n",
        "  def _longest_encoded_length(self):\n",
        "    longest_length = 0\n",
        "    for text in self.encoded_text:\n",
        "      longest_length = max(longest_length, len(text))\n",
        "    return longest_length\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    encoded = self.encoded_text[index]\n",
        "    label = self.data.iloc[index]['Label']\n",
        "    return (\n",
        "        torch.tensor(encoded, dtype=torch.long),\n",
        "        torch.tensor(label, dtype=torch.long)\n",
        "    )\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n"
      ],
      "metadata": {
        "id": "fnxO7da62CdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding('gpt2')"
      ],
      "metadata": {
        "id": "EKUEOKTA7Kwr"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SpamDataset(csv_file=\"Classification_finetuning/train.csv\", max_length=None, tokenizer=tokenizer)\n",
        "print(train_dataset.max_length)\n",
        "print(train_dataset.__len__())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6KHafGE7EyU",
        "outputId": "c1dd76fb-c2ad-4de5-cc33-76162d9d4bd0"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "257\n",
            "3900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset = SpamDataset(csv_file='Classification_finetuning/validation.csv', max_length=train_dataset.max_length, tokenizer=tokenizer)\n",
        "print(validation_dataset.max_length)\n",
        "print(validation_dataset.__len__())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU88G1O57imh",
        "outputId": "31149364-fc3f-4cdd-96df-d5a530ff772d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "257\n",
            "557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = SpamDataset(csv_file='Classification_finetuning/test.csv', max_length=train_dataset.max_length, tokenizer=tokenizer)\n",
        "print(test_dataset.max_length)\n",
        "print(test_dataset.__len__())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx0iyBCT8CIq",
        "outputId": "a51cce78-b4b9-4403-95ce-4dbfdcb6e35e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "257\n",
            "1115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "torch.manual_seed(123)\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True)\n",
        "validation_loader = DataLoader(\n",
        "    dataset=validation_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False)\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False)\n"
      ],
      "metadata": {
        "id": "YcnXJjP18OkY"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train dataloader')\n",
        "for input, target in train_loader:\n",
        "  pass\n",
        "print(input)\n",
        "print(input.shape)\n",
        "print(target)\n",
        "\n",
        "print(len(train_loader))\n",
        "print(len(validation_loader))\n",
        "print(len(test_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7X7Lp4N8zLs",
        "outputId": "7d2b096b-2f5a-4fdc-d3e8-b7be595ca51a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataloader\n",
            "tensor([[25374, 41649, 34509,  ..., 50256, 50256, 50256],\n",
            "        [10814,   986,  9576,  ..., 50256, 50256, 50256],\n",
            "        [39274,   337,  1546,  ..., 50256, 50256, 50256],\n",
            "        ...,\n",
            "        [ 8642,    23,    13,  ..., 50256, 50256, 50256],\n",
            "        [   44,  6996, 33826,  ..., 50256, 50256, 50256],\n",
            "        [ 2061,  1645,   284,  ..., 50256, 50256, 50256]])\n",
            "torch.Size([8, 257])\n",
            "tensor([0, 0, 1, 0, 0, 0, 0, 0])\n",
            "487\n",
            "70\n",
            "140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define GPT Architecture"
      ],
      "metadata": {
        "id": "hdW36l57-1U4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title GPT Model\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 1024,\n",
        "    \"emb_dim\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"qkv_bias\": True,\n",
        "}\n",
        "\n",
        "class MultiHeadAttention(torch.nn.Module):\n",
        "  def __init__(self, embedding_size, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out // num_heads\n",
        "\n",
        "    # Optimized for neural netowrk, much better for backward propogation\n",
        "    self.Wq = torch.nn.Linear(embedding_size, d_out, bias=qkv_bias)\n",
        "    self.Wk = torch.nn.Linear(embedding_size, d_out, bias=qkv_bias)\n",
        "    self.Wv = torch.nn.Linear(embedding_size, d_out, bias=qkv_bias)\n",
        "\n",
        "    # To combine head outputs\n",
        "    self.out_proj = torch.nn.Linear(d_out, d_out)\n",
        "\n",
        "    self.dropout = torch.nn.Dropout(dropout)\n",
        "    self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "  def forward(self, input_embeddings):\n",
        "    batch_size, context_length, embedding_size = input_embeddings.shape\n",
        "\n",
        "    Q = self.Wq(input_embeddings)\n",
        "    K = self.Wk(input_embeddings)\n",
        "    V = self.Wv(input_embeddings)\n",
        "\n",
        "    # Weight split\n",
        "    # Unroll the last dimention to include num_heads and head_dim\n",
        "    # (batch_size, context_length, d_out) -> (batch_size, context_length, num_heads, head_dim)\n",
        "    # 4-dimension tensor\n",
        "    Q = Q.view(batch_size, context_length, self.num_heads, self.head_dim)\n",
        "    K = K.view(batch_size, context_length, self.num_heads, self.head_dim)\n",
        "    V = V.view(batch_size, context_length, self.num_heads, self.head_dim)\n",
        "    # Group the matrics by \"number of heads\"\n",
        "    # (batch_size, context_length, num_heads, head_dim) ->\n",
        "    # (batch_size, num_heads, context_length, head_dim)\n",
        "    Q = Q.transpose(1, 2)\n",
        "    K = K.transpose(1, 2)\n",
        "    V = V.transpose(1, 2)\n",
        "\n",
        "    attention_scores = Q @ K.transpose(2, 3)\n",
        "\n",
        "    # mask, for causal attention\n",
        "    attention_scores.masked_fill_(\n",
        "        self.mask.bool()[:context_length, :context_length], -torch.inf\n",
        "    )\n",
        "\n",
        "    # scale and softmax\n",
        "    attention_weights = torch.softmax(attention_scores / self.head_dim**0.5, dim=-1)\n",
        "\n",
        "    # drop out, to avoid overfitting\n",
        "    attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "    # (batch_size, head_size, context_length, head_dim)\n",
        "    context_vectors = attention_weights @ V\n",
        "\n",
        "    context_vectors = context_vectors.transpose(1, 2)\n",
        "    # (batch_size, context_length, d_out)\n",
        "    # contiguous() to make sure they're in same memory block\n",
        "    context_vectors = context_vectors.contiguous().view(batch_size, context_length, self.d_out)\n",
        "\n",
        "    #print(context_vectors.shape)\n",
        "\n",
        "    context_vectors = self.out_proj(context_vectors)\n",
        "    return context_vectors\n",
        "\n",
        "class GPTModel(torch.nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb = torch.nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
        "    self.pos_emb = torch.nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
        "    self.dropout_layer = torch.nn.Dropout(cfg['drop_rate'])\n",
        "\n",
        "    self.trf_blocks = torch.nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
        "    self.final_norm = LayerNorm(cfg['emb_dim'])\n",
        "    self.out_head = torch.nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias=False)\n",
        "\n",
        "  # in_idx: sequences of token ids\n",
        "  def forward(self, in_idx):\n",
        "    batch_size, seq_len = in_idx.shape\n",
        "    tok_embeds = self.tok_emb(in_idx)\n",
        "    pos_embeds = self.pos_emb(torch.arange(0, seq_len, device=in_idx.device))\n",
        "    x = tok_embeds + pos_embeds\n",
        "    x = self.dropout_layer(x)\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits\n",
        "\n",
        "\n",
        "class TransformerBlock(torch.nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.norm1 = LayerNorm(cfg['emb_dim'])\n",
        "    self.norm2 = LayerNorm(cfg['emb_dim'])\n",
        "    # embedding_size, d_out, context_length, dropout, num_heads, qkv_bias=False\n",
        "    self.att = MultiHeadAttention(cfg['emb_dim'], cfg['emb_dim'], cfg['context_length'],\n",
        "                                        cfg['drop_rate'], cfg['n_heads'], cfg['qkv_bias'])\n",
        "    self.ff = FeedForward(cfg)\n",
        "    self.dropout = torch.nn.Dropout(cfg['drop_rate'])\n",
        "\n",
        "    return\n",
        "\n",
        "  def forward(self, x):\n",
        "    shortcut = x\n",
        "    x = self.norm1(x)\n",
        "    x = self.att(x)\n",
        "    x = self.dropout(x)\n",
        "    x = x + shortcut\n",
        "\n",
        "    shortcut = x\n",
        "    x = self.norm2(x)\n",
        "    x = self.ff(x)\n",
        "    x = self.dropout(x)\n",
        "    x = x + shortcut\n",
        "    return x\n",
        "\n",
        "class LayerNorm(torch.nn.Module):\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    # trainable parameters\n",
        "    # allows the model to learn appropriate scaling and shifting that best suit the data\n",
        "    self.scale = torch.nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = torch.nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self, input_embeddings):\n",
        "    mean = input_embeddings.mean(dim = -1, keepdim=True)\n",
        "    # If unbiased=True, apply Bessel's correction\n",
        "    var = input_embeddings.var(dim = -1, keepdim=True, unbiased=False)\n",
        "    # Add a small eps to avoid divide-by-zero\n",
        "    norm_input_embedding = (input_embeddings - mean) / torch.sqrt(var + self.eps)\n",
        "    return self.scale * norm_input_embedding + self.shift\n",
        "\n",
        "class GELU(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, input_embeddings):\n",
        "    return 0.5 * input_embeddings * (1 + torch.tanh(\n",
        "        torch.sqrt(torch.tensor(2.0 / torch.pi)) * (input_embeddings + 0.044715 * torch.pow(input_embeddings, 3))\n",
        "    ))\n",
        "\n",
        "class FeedForward(torch.nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.layers = torch.nn.Sequential(\n",
        "        # Expansion\n",
        "        torch.nn.Linear(cfg['emb_dim'], 4 * cfg['emb_dim']),\n",
        "        # Activation\n",
        "        GELU(),\n",
        "        # Contraction\n",
        "        torch.nn.Linear(4 * cfg['emb_dim'], cfg['emb_dim']),\n",
        "    )\n",
        "\n",
        "  def forward(self, input_embeddings):\n",
        "    return self.layers(input_embeddings)"
      ],
      "metadata": {
        "id": "arNIRqB9JU5W"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Device\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "  print('cuda')\n",
        "elif torch.backends.mps.is_available():\n",
        "  device = torch.device('mps')\n",
        "  print('mps')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "  print('cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81OEWWQuLyRM",
        "outputId": "a7928206-5b5f-4621-bd6f-101b13f57915"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate text util\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "  return torch.tensor([tokenizer.encode(text)])\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "  return tokenizer.decode(token_ids[0].squeeze(0).tolist())\n",
        "\n",
        "def generate_text_top_k(model, input_idx, max_new_tokens, context_size, temperature, top_k=5, eos_id=None):\n",
        "\n",
        "  # Generate up to max_new_tokens of tokens\n",
        "  for _ in range(max_new_tokens):\n",
        "    # get the last context_size tokens if the input is longer than context_size\n",
        "    idx_cond = input_idx[:, -context_size:]\n",
        "\n",
        "    # Step 1: produce the output logits\n",
        "    with torch.no_grad():\n",
        "      logits = model(idx_cond)\n",
        "\n",
        "    # Step 2: get the last vector of the logits\n",
        "    logits = logits[:, -1, :]\n",
        "\n",
        "\n",
        "    # Step 2.1: top K\n",
        "    top_logits, _ = torch.topk(logits, top_k)\n",
        "    logits = torch.where(\n",
        "        condition=logits < top_logits[:, -1],\n",
        "        input=torch.tensor(float('-inf')).to(logits.device),\n",
        "        other=logits\n",
        "    )\n",
        "\n",
        "    # Step 2.2: temperature scaling\n",
        "    logits = logits / temperature\n",
        "\n",
        "    # Step 3: get probability from logits using softmax\n",
        "    prob = torch.softmax(logits, dim=-1)  # (batch_size, vocab_size)\n",
        "\n",
        "    # Step 4: sample next token according to multinomial distribution\n",
        "    predicted_tokens = torch.multinomial(prob, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "    # Stop if end of sequence encountered\n",
        "    if predicted_tokens[0] == eos_id:\n",
        "      break\n",
        "\n",
        "    # Step 5: append the predicted token to the previous input tokens\n",
        "    input_idx = torch.cat((input_idx, predicted_tokens), dim=1)  # (batch_size, n_tokens + 1)\n",
        "\n",
        "  return input_idx"
      ],
      "metadata": {
        "id": "0zDPQ7EGLGUQ"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_text_generation(gpt, prompt):\n",
        "  torch.manual_seed(123)\n",
        "\n",
        "  tokenizer = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "  token_ids = generate_text_top_k(\n",
        "      model=gpt,\n",
        "      input_idx=text_to_token_ids(prompt, tokenizer).to(device),\n",
        "      max_new_tokens=100,\n",
        "      context_size=GPT_CONFIG_124M[\"context_length\"],\n",
        "      top_k=50,\n",
        "      temperature=1.4,\n",
        "      eos_id=GPT_CONFIG_124M['vocab_size'] - 1\n",
        "  )\n",
        "\n",
        "  print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "id": "V0IlDzH_LUnu"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Pre-trained GPT weights"
      ],
      "metadata": {
        "id": "z5R4PLD9L4OR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download GPT2 setting and config\n",
        "from gpt_download import download_and_load_gpt2\n",
        "\n",
        "model_size = '124M'\n",
        "models_dir=\"gpt2\"\n",
        "\n",
        "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
        "\n",
        "print(\"Settings:\", settings)\n",
        "print(\"Parameter dictionary keys:\", params.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jyt92rBMvkB",
        "outputId": "b5258b93-b093-4570-ab2b-f22780b5ea3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "checkpoint: 100%|███████████████████████████████████████████████████████████████████| 77.0/77.0 [00:00<00:00, 23.7kiB/s]\n",
            "encoder.json: 100%|███████████████████████████████████████████████████████████████| 1.04M/1.04M [00:00<00:00, 2.72MiB/s]\n",
            "hparams.json: 100%|█████████████████████████████████████████████████████████████████| 90.0/90.0 [00:00<00:00, 46.0kiB/s]\n",
            "model.ckpt.data-00000-of-00001: 100%|███████████████████████████████████████████████| 498M/498M [01:13<00:00, 6.78MiB/s]\n",
            "model.ckpt.index: 100%|███████████████████████████████████████████████████████████| 5.21k/5.21k [00:00<00:00, 1.09MiB/s]\n",
            "model.ckpt.meta: 100%|██████████████████████████████████████████████████████████████| 471k/471k [00:00<00:00, 1.75MiB/s]\n",
            "vocab.bpe: 100%|████████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 1.73MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
            "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load weights\n",
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
        "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
        "\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.Wq.weight = assign(\n",
        "            gpt.trf_blocks[b].att.Wq.weight, q_w.T)\n",
        "        gpt.trf_blocks[b].att.Wk.weight = assign(\n",
        "            gpt.trf_blocks[b].att.Wk.weight, k_w.T)\n",
        "        gpt.trf_blocks[b].att.Wv.weight = assign(\n",
        "            gpt.trf_blocks[b].att.Wv.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.Wq.bias = assign(\n",
        "            gpt.trf_blocks[b].att.Wq.bias, q_b)\n",
        "        gpt.trf_blocks[b].att.Wk.bias = assign(\n",
        "            gpt.trf_blocks[b].att.Wk.bias, k_b)\n",
        "        gpt.trf_blocks[b].att.Wv.bias = assign(\n",
        "            gpt.trf_blocks[b].att.Wv.bias, v_b)\n",
        "\n",
        "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].norm1.scale = assign(\n",
        "            gpt.trf_blocks[b].norm1.scale,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm1.shift = assign(\n",
        "            gpt.trf_blocks[b].norm1.shift,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.trf_blocks[b].norm2.scale = assign(\n",
        "            gpt.trf_blocks[b].norm2.scale,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm2.shift = assign(\n",
        "            gpt.trf_blocks[b].norm2.shift,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
      ],
      "metadata": {
        "id": "RwyBqJB-Ju3R"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt = GPTModel(GPT_CONFIG_124M)\n",
        "load_weights_into_gpt(gpt, params)"
      ],
      "metadata": {
        "id": "zeOxYtZ0Jnl8"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt.to(device)\n",
        "gpt.eval();\n",
        "\n",
        "test_text_generation(gpt, \"how are you?\")"
      ],
      "metadata": {
        "id": "roKANxuhZik_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfb38a12-dcc4-4aea-ce17-acc3e70e4e8a"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " how are you? I don't go into any names like you are and nobody else even understands you,\" she explains. For months the pair grew increasingly frustrated by the amount of criticism they've received, but now, as she speaks to her colleagues, they know they still get that backlash: \"I always know that some people I have worked with hate on my eyes. When I talk about it with them at dinner, some say the negative things I say about how I look and sometimes when I use words to describe other\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Re-architecture the GPT model for classification FT"
      ],
      "metadata": {
        "id": "xJ__dGMzMEBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace the output head\n",
        "# We could technically use a single output head, but that requires modifying the loss function.\n",
        "# We choose a more general approach where the number of output nodes matches the number of classes.\n",
        "\n",
        "# First, freeze all the parameters in the model\n",
        "for param in gpt.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# This out_head has requires_grad = True by default\n",
        "num_classes = 2\n",
        "gpt.out_head = torch.nn.Linear(in_features=GPT_CONFIG_124M['emb_dim'], out_features=num_classes)\n",
        "\n",
        "# Unfree the last transformer block and the last layer norm\n",
        "for param in gpt.trf_blocks[-1].parameters():\n",
        "  param.requires_grad = True\n",
        "for param in gpt.final_norm.parameters():\n",
        "  param.requires_grad = True\n"
      ],
      "metadata": {
        "id": "UjwPHr_pChsU"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "with torch.no_grad():\n",
        "  outputs = gpt(text_to_token_ids(\"how are you\", tokenizer).to(device))\n",
        "\n",
        "print(f\"Output: {outputs}\\ndimensions: {outputs.shape}\")"
      ],
      "metadata": {
        "id": "TRSFQEqJIWZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "304610ff-6af3-43dd-c252-c525656c6643"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: tensor([[[-1.4889,  1.0775],\n",
            "         [-1.6432,  5.4380],\n",
            "         [-2.9049,  5.9402]]], device='mps:0')\n",
            "dimensions: torch.Size([1, 3, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Function"
      ],
      "metadata": {
        "id": "awXDMiTcJJiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r_4I5lN_JO04"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}