{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "import os\n",
        "import re\n"
      ],
      "metadata": {
        "id": "b64TJUQWgUoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SimpleTokenzierV1"
      ],
      "metadata": {
        "id": "zLpMb5c2jI0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for item in os.listdir('.'): # '.' refers to the current directory\n",
        "#     print(item)\n",
        "\n",
        "os.getcwd()"
      ],
      "metadata": {
        "id": "nVjdmc09dmcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_mlE3ClQbeA"
      },
      "outputs": [],
      "source": [
        "with open('the-verdict.txt', 'r', encoding='utf-8') as f:\n",
        "  raw_txt = f.read()\n",
        "\n",
        "print(len(raw_txt))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove whitespace or not?\n",
        "#   Removing whitespaces reduces memory and computing requirement\n",
        "#   White spaces can be useful for text sensitive to the structure, like python indention\n",
        "preprossed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_txt)\n",
        "preprossed = [t.strip() for t in preprossed if t.strip()]\n",
        "print(preprossed[:10])\n",
        "print(len(preprossed))"
      ],
      "metadata": {
        "id": "Vz6HQH1ZgQqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocabulary: all the unique tokens in alphbetically order\n",
        "\n",
        "\n",
        "# For tokens not in the vocab\n",
        "UNKNOWN_TOKEN = \"<|unk|>\"\n",
        "\n",
        "# Added between text sources.\n",
        "# Allow the LLM to process and understand the data better.\n",
        "END_OF_TEXT_TOKEN = \"<|endoftext|>\"\n",
        "\n",
        "# Following special tokens are used by different types of tokenizers\n",
        "# [BOS]: beginning of sequence\n",
        "# [EOS]: end of sequence\n",
        "# [PAD]: padding\n",
        "\n",
        "sorted_unique_tokens = sorted(set(preprossed))\n",
        "sorted_unique_tokens.extend([END_OF_TEXT_TOKEN, UNKNOWN_TOKEN])\n",
        "print(len(sorted_unique_tokens))"
      ],
      "metadata": {
        "id": "9HV7FQ0hh1dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode token to token id\n",
        "vocab = {token:id for id,token in enumerate(sorted_unique_tokens)}\n",
        "\n",
        "# for i, item in enumerate(vocab.items()):\n",
        "#   print(i, item)\n",
        "#   if i > 20:\n",
        "#     break"
      ],
      "metadata": {
        "id": "Pnra95vEkSJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenzierV1:\n",
        "  def __init__(self, vocab):\n",
        "    self.token_to_id = vocab\n",
        "    self.id_to_token = {id:token for token,id in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [t.strip() for t in preprocessed if t.strip()]\n",
        "    preprocessed = [t if t in self.token_to_id else UNKNOWN_TOKEN for t in preprocessed]\n",
        "    return [self.token_to_id[t] for t in preprocessed]\n",
        "\n",
        "  def decode(self, ids):\n",
        "    tokens = [self.id_to_token[id] for id in ids]\n",
        "    text = \" \".join(tokens)\n",
        "    return re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)"
      ],
      "metadata": {
        "id": "FKppEdWFlMXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_v1 = SimpleTokenzierV1(vocab)"
      ],
      "metadata": {
        "id": "-4orvWMmoGt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"how are you, jessica!\"\n",
        "text2 = \"do you like tea?\"\n",
        "test_ids = tokenizer_v1.encode(\" <|endoftext|> \".join((text1, text2)))\n",
        "print(test_ids)\n",
        "print(tokenizer_v1.decode(test_ids))"
      ],
      "metadata": {
        "id": "1gOPlTcDoLJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BPE Tokenizer"
      ],
      "metadata": {
        "id": "Xr6Lw-Pl6-Jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "\n",
        "# The immediate space preceding the word and the word itself are encoded as a single token\n",
        "\n",
        "END_OF_TEXT_TOKEN = \"<|endoftext|>\"\n",
        "\n",
        "tokenizer_bpe = tiktoken.get_encoding('gpt2') # download pre-trained vocabulary and merge rules\n",
        "\n",
        "# texts = [\"\", \"I'm\", \"I'm\"]\n",
        "# test_ids = tokenizer_bpe.encode(END_OF_TEXT_TOKEN.join(texts), allowed_special={END_OF_TEXT_TOKEN})\n",
        "# print(test_ids)\n",
        "# print(tokenizer_bpe.decode(test_ids))"
      ],
      "metadata": {
        "id": "PhY5LMQn6_mJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset and DataLoader\n"
      ],
      "metadata": {
        "id": "kd0HOIVmje3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class DatasetV1(Dataset):\n",
        "  def __init__(self, txt, tokenizer, max_length, stride):\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "\n",
        "    token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "    for i in range(0, len(token_ids) - max_length, stride):\n",
        "      # One pair contains max_length training targets\n",
        "      self.input_ids.append(torch.tensor(token_ids[i:i+max_length]))\n",
        "      self.target_ids.append(torch.tensor(token_ids[i+1:i+1+max_length]))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "# Dataload will load the dataset efficiently\n",
        "# batch_size: The data the model has to process before updating the parameters\n",
        "#             The number of tensor pairs each dataloader iteration return\n",
        "#             Smaller batch_size requires less memory but more noisy small updates.\n",
        "#             Larger batch_size will make less noisy updates but take more time.\n",
        "# max_length: The context length (the sliding window size)\n",
        "# drop_last:  To drop the last batch if it's shorter to prevent loss spike during training\n",
        "# stride: word overlapping will create overfitting, larger stride also help go through the text faster\n",
        "# num_workders: process the input in parallel\n",
        "def create_dataloader_v1(txt, batch_size, max_length, stride, shuffle=False, drop_last=True, num_workers=0):\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "  dataset = DatasetV1(txt, tokenizer, max_length, stride)\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "  return dataloader\n"
      ],
      "metadata": {
        "id": "Tm5GF5NkjiRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataloader = create_dataloader_v1(raw_txt)\n",
        "# data_iter = iter(dataloader)\n",
        "# first_batch = next(data_iter)\n",
        "# print(first_batch)"
      ],
      "metadata": {
        "id": "jkgZ_aEBowAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Token Embeddings and Position Embeddings"
      ],
      "metadata": {
        "id": "AfK4FMUDwrhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import gensim.downloader as api\n",
        "\n",
        "# # 300 dimension\n",
        "# # huggingface.co/fse/word2vec-google-news-300\n",
        "# # Download the vector\n",
        "# word_vectors = api.load(\"word2vec-google-news-300\")\n"
      ],
      "metadata": {
        "id": "IPWxxyMJwt4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(word_vectors['computer'])\n",
        "# print(word_vectors.most_similar(positive=['king', 'woman'], negative=['man'], topn=10))\n",
        "# print(word_vectors.similarity(['woman', 'man']))\n",
        "# print(word_vectors.similarity(['tokyo', 'kyoto']))\n",
        "# print(word_vectors.similarity(['fish', 'bicycle']))\n",
        "# print(np.linalg.norm(word_vectors['women'] - word_vectors['man']))\n",
        "# print(np.linalg.norm(word_vectors['snow'] - word_vectors['pixel']))"
      ],
      "metadata": {
        "id": "oaf4eVY8x5Yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Create a embedding layer weight matrix\n",
        "vocab_size = 50000\n",
        "embedding_size = 8\n",
        "context_length = 4\n",
        "batch_size = 2\n",
        "d_out = 15\n",
        "num_heads = 3\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# A simple lookup table that stores embedding of a fixed dictionary and size.\n",
        "# Initialized to random numbers.\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, embedding_size)\n",
        "\n",
        "# Position embedding weight matrix\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, embedding_size)\n",
        "\n",
        "\n",
        "# print(embedding_layer.weight)\n",
        "# print(embedding_layer(torch.tensor([3])))\n",
        "\n",
        "# input_ids = torch.tensor([2, 3, 5, 1])\n",
        "# print(embedding_layer(input_ids))"
      ],
      "metadata": {
        "id": "rYsKRYRE1F0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "with open('the-verdict.txt', 'r', encoding='utf-8') as f:\n",
        "  raw_txt = f.read()\n",
        "\n",
        "print(len(raw_txt))\n",
        "\n",
        "dataloader = create_dataloader_v1(raw_txt, batch_size=batch_size, max_length=context_length, stride=context_length)\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "\n",
        "\n",
        "token_embeddings = embedding_layer(inputs) # batch_size x context_length x embedding_size\n",
        "# print(token_embeddings.shape)\n",
        "# print(input)\n",
        "# print(torch.arange(0, context_length))\n",
        "pos_embeddings = pos_embedding_layer(torch.arange(0, context_length)) # context_length x embedding_size\n",
        "input_embeddings = token_embeddings + pos_embeddings # python broadcasting\n",
        "print(input_embeddings)"
      ],
      "metadata": {
        "id": "k8MjKWp02agy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self Attention"
      ],
      "metadata": {
        "id": "dAbSYJ3uUraC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SelfAttentionV1(torch.nn.Module):\n",
        "  def __init__(self, embedding_size, d_out):\n",
        "    super().__init__()\n",
        "    self.Wq = torch.nn.Parameter(torch.rand(embedding_size, d_out), requires_grad=False)\n",
        "    self.Wk = torch.nn.Parameter(torch.rand(embedding_size, d_out), requires_grad=False)\n",
        "    self.Wv = torch.nn.Parameter(torch.rand(embedding_size, d_out), requires_grad=False)\n",
        "\n",
        "  def forward(input_embeddings):\n",
        "    Q = input_embeddings @ self.Wq\n",
        "    K = input_embeddings @ self.Wk\n",
        "    V = input_embeddings @ self.Wv\n",
        "\n",
        "    print(Q.shape, K.shape, V.shape)\n",
        "\n",
        "    attention_scores = Q @ K.transpose(-1, -2)\n",
        "    attention_weights = torch.softmax(attention_scores / d_out**0.5, dim = -1)\n",
        "    context_vectors = attention_weights @ V\n",
        "\n",
        "    print(context_vectors[0])\n",
        "    return context_vectors\n"
      ],
      "metadata": {
        "id": "6Huigq8NUrCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SelfAttentionV2(torch.nn.Module):\n",
        "  def __init__(self, embedding_size, d_out, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.Wq = torch.nn.Linear(embedding_size, d_out, bias=qkv_bias)\n",
        "    self.Wk = torch.nn.Linear(embedding_size, d_out, bias=qkv_bias)\n",
        "    self.Wv = torch.nn.Linear(embedding_size, d_out, bias=qkv_bias)\n",
        "\n",
        "  def forward(input_embeddings):\n",
        "    Q = self.Wq(input_embeddings)\n",
        "    K = self.Wk(input_embeddings)\n",
        "    V = self.Wv(input_embeddings)\n",
        "\n",
        "    print(Q.shape, K.shape, V.shape)\n",
        "\n",
        "    attention_scores = Q @ K.transpose(-1, -2)\n",
        "    attention_weights = torch.softmax(attention_scores / d_out**0.5, dim = -1)\n",
        "    context_vectors = attention_weights @ V\n",
        "\n",
        "    print(context_vectors[0])\n",
        "    return context_vectors\n"
      ],
      "metadata": {
        "id": "zu8FaXUFvST0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttentionV1(torch.nn.Module):\n",
        "  def __init__(self, embedding_size, d_out, context_length, drop_out=0.2, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.Wq = torch.nn.Linear(embedding_size, d_out, bias=qkv_bias)\n",
        "    self.Wk = torch.nn.Linear(embedding_size, d_out, bias=qkv_bias)\n",
        "    self.Wv = torch.nn.Linear(embedding_size, d_out, bias=qkv_bias)\n",
        "    # dropout rate = 0.2\n",
        "    # * 20% of the elements in the matrix is switched to 0\n",
        "    # * the remaining values are scaled by 120%\n",
        "    self.dropout = torch.Dropout(drop_out)\n",
        "    # mask\n",
        "    self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagnol=1))\n",
        "\n",
        "  def forward(input_embeddings):\n",
        "    batch_size, context_length, embedding_size = input_embeddings.shape\n",
        "    d_out = self.Wq.shape[1]\n",
        "\n",
        "    Q = self.Wq(input_embeddings)\n",
        "    K = self.Wk(input_embeddings)\n",
        "    V = self.Wv(input_embeddings)\n",
        "\n",
        "    print(Q.shape, K.shape, V.shape)\n",
        "\n",
        "    attention_scores = Q @ K.transpose(1, 2)\n",
        "\n",
        "    # mask\n",
        "    # context_length = input_embeddings.shape[1]\n",
        "    # mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "    # masked = attention_scores.masked_fill(mask.bool(), -torch.inf)\n",
        "    # _ ops are in-place\n",
        "    # when the current batch has smaller context length than the mask's\n",
        "    attention_scores.masked_fill_(self.mask.bool()[:context_length, :context_length], -torch.inf)\n",
        "\n",
        "    # scale and softmax\n",
        "    attention_weights = torch.softmax(attention_scores / d_out**0.5, dim = -1)\n",
        "\n",
        "    # dropout\n",
        "    # Prevents over-fitting and improves generalization performance\n",
        "    attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "    # Data leakage: the weight is influenced by the masked attention weights during softmax\n",
        "    # triangular lower function\n",
        "    # self.mask = torch.tril(torch.ones(context_length, context_length))\n",
        "    # masked_attention_weights = attention_weights * self.mask\n",
        "    # row_sums = masked_attention_weights.sum(dim=1, keepdim=True)\n",
        "    # masked_attention_norm = masked_attention_weights / row_sums\n",
        "\n",
        "    context_vectors = attention_weights @ V\n",
        "\n",
        "    print(context_vectors[0])\n",
        "    return context_vectors"
      ],
      "metadata": {
        "id": "jq1anoAFB6Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ca = CasualAttentionV1(embedding_size, qkv_size, context_length, 0.2)\n",
        "context_vectors = ca(input_embeddings)"
      ],
      "metadata": {
        "id": "CiVVTzzJKxmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi Head attention\n",
        "\n",
        "# output is context length * num_heads\n",
        "class MultiHeadAttentionWrapper(torch.nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.heads = torch.nn.ModuleList([CausalAttentionV1(d_in, d_out, context_length, dropout, qkv_bias)\n",
        "                                for _ in range(num_heads)])\n",
        "\n",
        "  def forward(self, input_embedding):\n",
        "    # TODO: need parallization using weight splits\n",
        "    return torch.cat([head(input_embedding) for head in self.heads], dim=-1)"
      ],
      "metadata": {
        "id": "9kZH7T7_MiJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multi_head_attention = MultiHeadAttentionWrapper(embedding_size, qkv_size, context_length, 0.2, 4)\n",
        "concat_context_vectors = multi_head_attention(input_embeddings)\n",
        "\n",
        "print(concat_context_vectors.shape)\n",
        "print(concat_context_vectors[0])"
      ],
      "metadata": {
        "id": "l2JxBCZOW87e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Multi Head Attention with Weight Split\n",
        "\n",
        "class MultiHeadAttention(torch.nn.Module):\n",
        "  def __init__(self, embedding_size, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out // num_heads\n",
        "\n",
        "    # Optimized for nueral netowrk, much better for backward propogation\n",
        "    self.Wq = torch.nn.Linear(embedding_size, d_out, bias=qkv_bias)\n",
        "    self.Wk = torch.nn.Linear(embedding_size, d_out, bias=qkv_bias)\n",
        "    self.Wv = torch.nn.Linear(embedding_size, d_out, bias=qkv_bias)\n",
        "\n",
        "    # To combine head outputs\n",
        "    self.out_proj = torch.nn.Linear(d_out, d_out)\n",
        "\n",
        "    self.dropout = torch.nn.Dropout(dropout)\n",
        "    self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "  def forward(self, input_embeddings):\n",
        "    batch_size, context_length, embedding_size = input_embeddings.shape\n",
        "\n",
        "    Q = self.Wq(input_embeddings)\n",
        "    K = self.Wk(input_embeddings)\n",
        "    V = self.Wv(input_embeddings)\n",
        "\n",
        "    # Weight split\n",
        "    # Unroll the last dimention to include num_heads and head_dim\n",
        "    # (batch_size, context_length, d_out) -> (batch_size, context_length, num_heads, head_dim)\n",
        "    # 4-dimension tensor\n",
        "    Q = Q.view(batch_size, context_length, self.num_heads, self.head_dim)\n",
        "    K = K.view(batch_size, context_length, self.num_heads, self.head_dim)\n",
        "    V = V.view(batch_size, context_length, self.num_heads, self.head_dim)\n",
        "    # Group the matrics by \"number of heads\"\n",
        "    # (batch_size, context_length, num_heads, head_dim) ->\n",
        "    # (batch_size, num_heads, context_length, head_dim)\n",
        "    Q = Q.transpose(1, 2)\n",
        "    K = K.transpose(1, 2)\n",
        "    V = V.transpose(1, 2)\n",
        "\n",
        "    attention_scores = Q @ K.transpose(2, 3)\n",
        "\n",
        "    # mask, for causal attention\n",
        "    attention_scores.masked_fill_(\n",
        "        self.mask.bool()[:context_length, :context_length], -torch.inf\n",
        "    )\n",
        "\n",
        "    # scale and softmax\n",
        "    attention_weights = torch.softmax(attention_scores / self.head_dim**0.5, dim=-1)\n",
        "\n",
        "    # drop out, to avoid overfitting\n",
        "    attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "    # (batch_size, head_size, context_length, head_dim)\n",
        "    context_vectors = attention_weights @ V\n",
        "\n",
        "    context_vectors = context_vectors.transpose(1, 2)\n",
        "    # (batch_size, context_length, d_out)\n",
        "    # contiguous() to make sure they're in same memory block\n",
        "    context_vectors = context_vectors.contiguous().view(batch_size, context_length, self.d_out)\n",
        "\n",
        "    print(context_vectors.shape)\n",
        "\n",
        "    context_vectors = self.out_proj(context_vectors)\n",
        "    return context_vectors\n",
        "\n"
      ],
      "metadata": {
        "id": "K0KhQkRtc_eB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mha = MultiHeadAttention(embedding_size, d_out, context_length, 0.1, num_heads)\n",
        "context_vectors = mha(input_embeddings)\n",
        "\n",
        "print(context_vectors.shape)\n",
        "print(context_vectors)"
      ],
      "metadata": {
        "id": "Td1oyaWToMqj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}