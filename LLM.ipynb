{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "zLpMb5c2jI0n",
        "Xr6Lw-Pl6-Jx",
        "kd0HOIVmje3s",
        "qJ4RrDz00aKv"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "import os\n",
        "import re\n",
        "\n",
        "import tiktoken\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "#import gensim.downloader as api\n"
      ],
      "metadata": {
        "id": "b64TJUQWgUoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [X] SimpleTokenzierV1"
      ],
      "metadata": {
        "id": "zLpMb5c2jI0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for item in os.listdir('.'): # '.' refers to the current directory\n",
        "#     print(item)\n",
        "\n",
        "os.getcwd()"
      ],
      "metadata": {
        "id": "nVjdmc09dmcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_mlE3ClQbeA"
      },
      "outputs": [],
      "source": [
        "with open('the-verdict.txt', 'r', encoding='utf-8') as f:\n",
        "  raw_txt = f.read()\n",
        "\n",
        "print(len(raw_txt))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove whitespace or not?\n",
        "#   Removing whitespaces reduces memory and computing requirement\n",
        "#   White spaces can be useful for text sensitive to the structure, like python indention\n",
        "preprossed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_txt)\n",
        "preprossed = [t.strip() for t in preprossed if t.strip()]\n",
        "print(preprossed[:10])\n",
        "print(len(preprossed))"
      ],
      "metadata": {
        "id": "Vz6HQH1ZgQqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocabulary: all the unique tokens in alphbetically order\n",
        "\n",
        "\n",
        "# For tokens not in the vocab\n",
        "UNKNOWN_TOKEN = \"<|unk|>\"\n",
        "\n",
        "# Added between text sources.\n",
        "# Allow the LLM to process and understand the data better.\n",
        "END_OF_TEXT_TOKEN = \"<|endoftext|>\"\n",
        "\n",
        "# Following special tokens are used by different types of tokenizers\n",
        "# [BOS]: beginning of sequence\n",
        "# [EOS]: end of sequence\n",
        "# [PAD]: padding\n",
        "\n",
        "sorted_unique_tokens = sorted(set(preprossed))\n",
        "sorted_unique_tokens.extend([END_OF_TEXT_TOKEN, UNKNOWN_TOKEN])\n",
        "print(len(sorted_unique_tokens))"
      ],
      "metadata": {
        "id": "9HV7FQ0hh1dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode token to token id\n",
        "vocab = {token:id for id,token in enumerate(sorted_unique_tokens)}\n",
        "\n",
        "# for i, item in enumerate(vocab.items()):\n",
        "#   print(i, item)\n",
        "#   if i > 20:\n",
        "#     break"
      ],
      "metadata": {
        "id": "Pnra95vEkSJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenzierV1:\n",
        "  def __init__(self, vocab):\n",
        "    self.token_to_id = vocab\n",
        "    self.id_to_token = {id:token for token,id in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [t.strip() for t in preprocessed if t.strip()]\n",
        "    preprocessed = [t if t in self.token_to_id else UNKNOWN_TOKEN for t in preprocessed]\n",
        "    return [self.token_to_id[t] for t in preprocessed]\n",
        "\n",
        "  def decode(self, ids):\n",
        "    tokens = [self.id_to_token[id] for id in ids]\n",
        "    text = \" \".join(tokens)\n",
        "    return re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)"
      ],
      "metadata": {
        "id": "FKppEdWFlMXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_v1 = SimpleTokenzierV1(vocab)"
      ],
      "metadata": {
        "id": "-4orvWMmoGt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"how are you, jessica!\"\n",
        "text2 = \"do you like tea?\"\n",
        "test_ids = tokenizer_v1.encode(\" <|endoftext|> \".join((text1, text2)))\n",
        "print(test_ids)\n",
        "print(tokenizer_v1.decode(test_ids))"
      ],
      "metadata": {
        "id": "1gOPlTcDoLJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [X] BPE Tokenizer"
      ],
      "metadata": {
        "id": "Xr6Lw-Pl6-Jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The immediate space preceding the word and the word itself are encoded as a single token\n",
        "\n",
        "END_OF_TEXT_TOKEN = \"<|endoftext|>\"\n",
        "\n",
        "tokenizer_bpe = tiktoken.get_encoding('gpt2') # download pre-trained vocabulary and merge rules\n",
        "\n",
        "# texts = [\"\", \"I'm\", \"I'm\"]\n",
        "# test_ids = tokenizer_bpe.encode(END_OF_TEXT_TOKEN.join(texts), allowed_special={END_OF_TEXT_TOKEN})\n",
        "# print(test_ids)\n",
        "# print(tokenizer_bpe.decode(test_ids))"
      ],
      "metadata": {
        "id": "PhY5LMQn6_mJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset and DataLoader\n"
      ],
      "metadata": {
        "id": "kd0HOIVmje3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetV1(Dataset):\n",
        "  def __init__(self, txt, tokenizer, max_length, stride):\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "\n",
        "    token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "    for i in range(0, len(token_ids) - max_length, stride):\n",
        "      # One pair contains max_length training targets\n",
        "      self.input_ids.append(torch.tensor(token_ids[i:i+max_length]))\n",
        "      self.target_ids.append(torch.tensor(token_ids[i+1:i+1+max_length]))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "# Dataload will load the dataset efficiently\n",
        "# batch_size: The data the model has to process before updating the parameters\n",
        "#             The number of tensor pairs each dataloader iteration return\n",
        "#             Smaller batch_size requires less memory but more noisy small updates.\n",
        "#             Larger batch_size will make less noisy updates but take more time.\n",
        "# max_length: The context length (the sliding window size)\n",
        "# drop_last:  To drop the last batch if it's shorter to prevent loss spike during training\n",
        "# stride: word overlapping will create overfitting, larger stride also help go through the text faster\n",
        "# num_workders: process the input in parallel\n",
        "def create_dataloader_v1(txt, batch_size, max_length, stride, shuffle=False, drop_last=True, num_workers=0):\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "  dataset = DatasetV1(txt, tokenizer, max_length, stride)\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "  return dataloader\n"
      ],
      "metadata": {
        "id": "Tm5GF5NkjiRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('the-verdict.txt', 'r', encoding='utf-8') as f:\n",
        "  raw_txt = f.read()\n",
        "\n",
        "print(len(raw_txt))\n",
        "\n",
        "dataloader = create_dataloader_v1(raw_txt, batch_size=2, max_length=10, stride=10)\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "\n",
        "print(inputs)"
      ],
      "metadata": {
        "id": "jkgZ_aEBowAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [X] Token Embeddings and Position Embeddings"
      ],
      "metadata": {
        "id": "AfK4FMUDwrhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 300 dimension\n",
        "# # huggingface.co/fse/word2vec-google-news-300\n",
        "# # Download the vector\n",
        "# word_vectors = api.load(\"word2vec-google-news-300\")\n"
      ],
      "metadata": {
        "id": "IPWxxyMJwt4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(word_vectors['computer'])\n",
        "# print(word_vectors.most_similar(positive=['king', 'woman'], negative=['man'], topn=10))\n",
        "# print(word_vectors.similarity(['woman', 'man']))\n",
        "# print(word_vectors.similarity(['tokyo', 'kyoto']))\n",
        "# print(word_vectors.similarity(['fish', 'bicycle']))\n",
        "# print(np.linalg.norm(word_vectors['women'] - word_vectors['man']))\n",
        "# print(np.linalg.norm(word_vectors['snow'] - word_vectors['pixel']))"
      ],
      "metadata": {
        "id": "oaf4eVY8x5Yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a embedding layer weight matrix\n",
        "# vocab_size = 50000\n",
        "embedding_size = 8\n",
        "context_length = 4\n",
        "batch_size = 2\n",
        "d_out = 15\n",
        "num_heads = 3\n",
        "\n",
        "# torch.manual_seed(123)\n",
        "\n",
        "# # A simple lookup table that stores embedding of a fixed dictionary and size.\n",
        "# # Initialized to random numbers.\n",
        "# embedding_layer = torch.nn.Embedding(vocab_size, embedding_size)\n",
        "\n",
        "# # Position embedding weight matrix\n",
        "# pos_embedding_layer = torch.nn.Embedding(context_length, embedding_size)\n",
        "\n",
        "\n",
        "# print(embedding_layer.weight)\n",
        "# print(embedding_layer(torch.tensor([3])))\n",
        "\n",
        "# input_ids = torch.tensor([2, 3, 5, 1])\n",
        "# print(embedding_layer(input_ids))"
      ],
      "metadata": {
        "id": "rYsKRYRE1F0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('the-verdict.txt', 'r', encoding='utf-8') as f:\n",
        "  raw_txt = f.read()\n",
        "\n",
        "print(len(raw_txt))\n",
        "\n",
        "dataloader = create_dataloader_v1(raw_txt, batch_size=batch_size, max_length=context_length, stride=context_length)\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "\n",
        "\n",
        "token_embeddings = embedding_layer(inputs) # batch_size x context_length x embedding_size\n",
        "# print(token_embeddings.shape)\n",
        "# print(input)\n",
        "# print(torch.arange(0, context_length))\n",
        "pos_embeddings = pos_embedding_layer(torch.arange(0, context_length)) # context_length x embedding_size\n",
        "input_embeddings = token_embeddings + pos_embeddings # python broadcasting\n",
        "print(input_embeddings)"
      ],
      "metadata": {
        "id": "k8MjKWp02agy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self Attention"
      ],
      "metadata": {
        "id": "dAbSYJ3uUraC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionV1(torch.nn.Module):\n",
        "  def __init__(self, embedding_size, d_out):\n",
        "    super().__init__()\n",
        "    self.Wq = torch.nn.Parameter(torch.rand(embedding_size, d_out), requires_grad=False)\n",
        "    self.Wk = torch.nn.Parameter(torch.rand(embedding_size, d_out), requires_grad=False)\n",
        "    self.Wv = torch.nn.Parameter(torch.rand(embedding_size, d_out), requires_grad=False)\n",
        "\n",
        "  def forward(input_embeddings):\n",
        "    Q = input_embeddings @ self.Wq\n",
        "    K = input_embeddings @ self.Wk\n",
        "    V = input_embeddings @ self.Wv\n",
        "\n",
        "    print(Q.shape, K.shape, V.shape)\n",
        "\n",
        "    attention_scores = Q @ K.transpose(-1, -2)\n",
        "    attention_weights = torch.softmax(attention_scores / d_out**0.5, dim = -1)\n",
        "    context_vectors = attention_weights @ V\n",
        "\n",
        "    print(context_vectors[0])\n",
        "    return context_vectors\n"
      ],
      "metadata": {
        "id": "6Huigq8NUrCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionV2(torch.nn.Module):\n",
        "  def __init__(self, embedding_size, d_out, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.Wq = torch.nn.Linear(embedding_size, d_out, bias=qkv_bias)\n",
        "    self.Wk = torch.nn.Linear(embedding_size, d_out, bias=qkv_bias)\n",
        "    self.Wv = torch.nn.Linear(embedding_size, d_out, bias=qkv_bias)\n",
        "\n",
        "  def forward(input_embeddings):\n",
        "    Q = self.Wq(input_embeddings)\n",
        "    K = self.Wk(input_embeddings)\n",
        "    V = self.Wv(input_embeddings)\n",
        "\n",
        "    print(Q.shape, K.shape, V.shape)\n",
        "\n",
        "    attention_scores = Q @ K.transpose(-1, -2)\n",
        "    attention_weights = torch.softmax(attention_scores / d_out**0.5, dim = -1)\n",
        "    context_vectors = attention_weights @ V\n",
        "\n",
        "    print(context_vectors[0])\n",
        "    return context_vectors\n"
      ],
      "metadata": {
        "id": "zu8FaXUFvST0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttentionV1(torch.nn.Module):\n",
        "  def __init__(self, embedding_size, d_out, context_length, drop_out=0.2, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.d_out = d_out\n",
        "    self.Wq = torch.nn.Linear(embedding_size, d_out, bias=qkv_bias)\n",
        "    self.Wk = torch.nn.Linear(embedding_size, d_out, bias=qkv_bias)\n",
        "    self.Wv = torch.nn.Linear(embedding_size, d_out, bias=qkv_bias)\n",
        "    # dropout rate = 0.2\n",
        "    # * 20% of the elements in the matrix is switched to 0\n",
        "    # * the remaining values are scaled by 120%\n",
        "    self.dropout = torch.nn.Dropout(drop_out)\n",
        "    # mask\n",
        "    self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "  def forward(self, input_embeddings):\n",
        "    batch_size, context_length, embedding_size = input_embeddings.shape\n",
        "\n",
        "    Q = self.Wq(input_embeddings)\n",
        "    K = self.Wk(input_embeddings)\n",
        "    V = self.Wv(input_embeddings)\n",
        "\n",
        "    print(Q.shape, K.shape, V.shape)\n",
        "\n",
        "    attention_scores = Q @ K.transpose(1, 2)\n",
        "\n",
        "    # mask\n",
        "    # context_length = input_embeddings.shape[1]\n",
        "    # mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "    # masked = attention_scores.masked_fill(mask.bool(), -torch.inf)\n",
        "    # _ ops are in-place\n",
        "    # when the current batch has smaller context length than the mask's\n",
        "    attention_scores.masked_fill_(self.mask.bool()[:context_length, :context_length], -torch.inf)\n",
        "\n",
        "    # scale and softmax\n",
        "    attention_weights = torch.softmax(attention_scores / d_out**0.5, dim = -1)\n",
        "\n",
        "    # dropout\n",
        "    # Prevents over-fitting and improves generalization performance\n",
        "    attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "    # Data leakage: the weight is influenced by the masked attention weights during softmax\n",
        "    # triangular lower function\n",
        "    # self.mask = torch.tril(torch.ones(context_length, context_length))\n",
        "    # masked_attention_weights = attention_weights * self.mask\n",
        "    # row_sums = masked_attention_weights.sum(dim=1, keepdim=True)\n",
        "    # masked_attention_norm = masked_attention_weights / row_sums\n",
        "\n",
        "    context_vectors = attention_weights @ V\n",
        "\n",
        "    print(context_vectors[0])\n",
        "    return context_vectors"
      ],
      "metadata": {
        "id": "jq1anoAFB6Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 8\n",
        "context_length = 10\n",
        "batch_size = 2\n",
        "d_out = 15\n",
        "num_heads = 3\n",
        "qkv_size = 5\n",
        "seq_len = 4\n",
        "dropout_rate = 0.2\n",
        "\n",
        "input_embeddings = torch.rand(batch_size, seq_len, embedding_size)\n",
        "ca = CausalAttentionV1(embedding_size, d_out=qkv_size, context_length=context_length, drop_out=dropout_rate)\n",
        "context_vectors = ca(input_embeddings)"
      ],
      "metadata": {
        "id": "CiVVTzzJKxmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi Head attention\n",
        "\n",
        "# output is context length * num_heads\n",
        "class MultiHeadAttentionWrapper(torch.nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.heads = torch.nn.ModuleList([CausalAttentionV1(d_in, d_out, context_length, dropout, qkv_bias)\n",
        "                                for _ in range(num_heads)])\n",
        "\n",
        "  def forward(self, input_embedding):\n",
        "    # TODO: need parallization using weight splits\n",
        "    return torch.cat([head(input_embedding) for head in self.heads], dim=-1)"
      ],
      "metadata": {
        "id": "9kZH7T7_MiJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multi_head_attention = MultiHeadAttentionWrapper(embedding_size, qkv_size, context_length, dropout_rate, num_heads)\n",
        "concat_context_vectors = multi_head_attention(input_embeddings)\n",
        "\n",
        "print(concat_context_vectors.shape)\n",
        "print(concat_context_vectors[0])"
      ],
      "metadata": {
        "id": "l2JxBCZOW87e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Masked Multi Head Attention with Weight Split\n",
        "\n",
        "class MultiHeadAttention(torch.nn.Module):\n",
        "  def __init__(self, embedding_size, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out // num_heads\n",
        "\n",
        "    # Optimized for neural netowrk, much better for backward propogation\n",
        "    self.Wq = torch.nn.Linear(embedding_size, d_out, bias=qkv_bias)\n",
        "    self.Wk = torch.nn.Linear(embedding_size, d_out, bias=qkv_bias)\n",
        "    self.Wv = torch.nn.Linear(embedding_size, d_out, bias=qkv_bias)\n",
        "\n",
        "    # To combine head outputs\n",
        "    self.out_proj = torch.nn.Linear(d_out, d_out)\n",
        "\n",
        "    self.dropout = torch.nn.Dropout(dropout)\n",
        "    self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "  def forward(self, input_embeddings):\n",
        "    batch_size, context_length, embedding_size = input_embeddings.shape\n",
        "\n",
        "    Q = self.Wq(input_embeddings)\n",
        "    K = self.Wk(input_embeddings)\n",
        "    V = self.Wv(input_embeddings)\n",
        "\n",
        "    # Weight split\n",
        "    # Unroll the last dimention to include num_heads and head_dim\n",
        "    # (batch_size, context_length, d_out) -> (batch_size, context_length, num_heads, head_dim)\n",
        "    # 4-dimension tensor\n",
        "    Q = Q.view(batch_size, context_length, self.num_heads, self.head_dim)\n",
        "    K = K.view(batch_size, context_length, self.num_heads, self.head_dim)\n",
        "    V = V.view(batch_size, context_length, self.num_heads, self.head_dim)\n",
        "    # Group the matrics by \"number of heads\"\n",
        "    # (batch_size, context_length, num_heads, head_dim) ->\n",
        "    # (batch_size, num_heads, context_length, head_dim)\n",
        "    Q = Q.transpose(1, 2)\n",
        "    K = K.transpose(1, 2)\n",
        "    V = V.transpose(1, 2)\n",
        "\n",
        "    attention_scores = Q @ K.transpose(2, 3)\n",
        "\n",
        "    # mask, for causal attention\n",
        "    attention_scores.masked_fill_(\n",
        "        self.mask.bool()[:context_length, :context_length], -torch.inf\n",
        "    )\n",
        "\n",
        "    # scale and softmax\n",
        "    attention_weights = torch.softmax(attention_scores / self.head_dim**0.5, dim=-1)\n",
        "\n",
        "    # drop out, to avoid overfitting\n",
        "    attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "    # (batch_size, head_size, context_length, head_dim)\n",
        "    context_vectors = attention_weights @ V\n",
        "\n",
        "    context_vectors = context_vectors.transpose(1, 2)\n",
        "    # (batch_size, context_length, d_out)\n",
        "    # contiguous() to make sure they're in same memory block\n",
        "    context_vectors = context_vectors.contiguous().view(batch_size, context_length, self.d_out)\n",
        "\n",
        "    #print(context_vectors.shape)\n",
        "\n",
        "    context_vectors = self.out_proj(context_vectors)\n",
        "    return context_vectors\n",
        "\n"
      ],
      "metadata": {
        "id": "K0KhQkRtc_eB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mha = MultiHeadAttention(embedding_size, d_out, context_length, 0.1, num_heads)\n",
        "context_vectors = mha(input_embeddings)\n",
        "\n",
        "print(context_vectors.shape)\n",
        "print(context_vectors)"
      ],
      "metadata": {
        "id": "Td1oyaWToMqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2"
      ],
      "metadata": {
        "id": "BOM8nVG-2nqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 256, #1024,\n",
        "    \"emb_dim\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"qkv_bias\": False,\n",
        "}\n",
        "\n",
        "class GPTModel(torch.nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb = torch.nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
        "    self.pos_emb = torch.nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
        "    self.dropout_layer = torch.nn.Dropout(cfg['drop_rate'])\n",
        "\n",
        "    self.trf_blocks = torch.nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
        "    self.final_norm = LayerNorm(cfg['emb_dim'])\n",
        "    self.out_head = torch.nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias=False)\n",
        "\n",
        "  # in_idx: sequences of token ids\n",
        "  def forward(self, in_idx):\n",
        "    batch_size, seq_len = in_idx.shape\n",
        "    tok_embeds = self.tok_emb(in_idx)\n",
        "    pos_embeds = self.pos_emb(torch.arange(0, seq_len, device=in_idx.device))\n",
        "    x = tok_embeds + pos_embeds\n",
        "    x = self.dropout_layer(x)\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits\n",
        "\n",
        "\n",
        "class TransformerBlock(torch.nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.norm1 = LayerNorm(cfg['emb_dim'])\n",
        "    self.norm2 = LayerNorm(cfg['emb_dim'])\n",
        "    # embedding_size, d_out, context_length, dropout, num_heads, qkv_bias=False\n",
        "    self.attention = MultiHeadAttention(cfg['emb_dim'], cfg['emb_dim'], cfg['context_length'],\n",
        "                                        cfg['drop_rate'], cfg['n_heads'], cfg['qkv_bias'])\n",
        "    self.ffn = FeedForward(cfg)\n",
        "    self.dropout = torch.nn.Dropout(cfg['drop_rate'])\n",
        "\n",
        "    return\n",
        "\n",
        "  def forward(self, x):\n",
        "    shortcut = x\n",
        "    x = self.norm1(x)\n",
        "    x = self.attention(x)\n",
        "    x = self.dropout(x)\n",
        "    x = x + shortcut\n",
        "\n",
        "    shortcut = x\n",
        "    x = self.norm2(x)\n",
        "    x = self.ffn(x)\n",
        "    x = self.dropout(x)\n",
        "    x = x + shortcut\n",
        "    return x\n",
        "\n",
        "class LayerNorm(torch.nn.Module):\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    # trainable parameters\n",
        "    # allows the model to learn appropriate scaling and shifting that best suit the data\n",
        "    self.scale = torch.nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = torch.nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self, input_embeddings):\n",
        "    mean = input_embeddings.mean(dim = -1, keepdim=True)\n",
        "    # If unbiased=True, apply Bessel's correction\n",
        "    var = input_embeddings.var(dim = -1, keepdim=True, unbiased=False)\n",
        "    # Add a small eps to avoid divide-by-zero\n",
        "    norm_input_embedding = (input_embeddings - mean) / torch.sqrt(var + self.eps)\n",
        "    return self.scale * norm_input_embedding + self.shift\n",
        "\n",
        "class GELU(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, input_embeddings):\n",
        "    return 0.5 * input_embeddings * (1 + torch.tanh(\n",
        "        torch.sqrt(torch.tensor(2.0 / torch.pi)) * (input_embeddings + 0.044715 * torch.pow(input_embeddings, 3))\n",
        "    ))\n",
        "\n",
        "class FeedForward(torch.nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.layers = torch.nn.Sequential(\n",
        "        # Expansion\n",
        "        torch.nn.Linear(cfg['emb_dim'], 4 * cfg['emb_dim']),\n",
        "        # Activation\n",
        "        GELU(),\n",
        "        # Contraction\n",
        "        torch.nn.Linear(4 * cfg['emb_dim'], cfg['emb_dim']),\n",
        "    )\n",
        "\n",
        "  def forward(self, input_embeddings):\n",
        "    return self.layers(input_embeddings)\n",
        ""
      ],
      "metadata": {
        "id": "s3WS2LsI22Zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt = GPTModel(GPT_CONFIG_124M)\n",
        "outputs = gpt(inputs)\n",
        "print(outputs.shape)"
      ],
      "metadata": {
        "id": "nuw118P0DLS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test: test the transformer block\n",
        "trf = TransformerBlock(GPT_CONFIG_124M)\n",
        "torch.manual_seed(123)\n",
        "batch_example = torch.rand(2, 5, GPT_CONFIG_124M['emb_dim'])\n",
        "outputs = trf(batch_example)\n",
        "print(outputs.shape)"
      ],
      "metadata": {
        "id": "oYCrSe_OtxGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test: Number of parameteres\n",
        "total_params = sum(p.numel() for p in gpt.parameters())\n",
        "print(total_params)\n",
        "\n",
        "output_layer_params = sum(p.numel() for p in gpt.out_head.parameters())\n",
        "print(output_layer_params)"
      ],
      "metadata": {
        "id": "R_yVsyyVeQ8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test: Memory\n",
        "total_size_bytes = total_params * 4\n",
        "total_size_mb = total_size_bytes / (1024 * 1024)\n",
        "print(f'{total_size_mb:.2f} MB')"
      ],
      "metadata": {
        "id": "DNY_e8gqfjxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test: layer normalization example\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "torch.manual_seed(123)\n",
        "batch_example = torch.randn(2, 5)\n",
        "layer = torch.nn.Sequential(torch.nn.Linear(5, 6), torch.nn.ReLU())\n",
        "out = layer(batch_example)\n",
        "print(out)\n",
        "\n",
        "mean = out.mean(dim=-1, keepdim=True)\n",
        "var = out.var(dim=-1, keepdim=True, unbiased=False)\n",
        "print(mean, \"\\n\", var)\n",
        "\n",
        "out_norm = ((out - mean) / torch.sqrt(var))\n",
        "\n",
        "mean = out_norm.mean(dim=-1, keepdim=True)\n",
        "var = out_norm.var(dim=-1, keepdim=True, unbiased=False)\n",
        "print(mean, \"\\n\", var)"
      ],
      "metadata": {
        "id": "6SbQXpGpSYrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test: Shortcut Connection and Vanishing Gradient\n",
        "class ExampleDeepNeuralNetwork(torch.nn.Module):\n",
        "  def __init__(self, layer_sizes, use_shortcut):\n",
        "    super().__init__()\n",
        "    self.use_shortcut = use_shortcut\n",
        "    self.layers = torch.nn.ModuleList([\n",
        "        torch.nn.Sequential(torch.nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
        "        torch.nn.Sequential(torch.nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
        "        torch.nn.Sequential(torch.nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
        "        torch.nn.Sequential(torch.nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
        "        torch.nn.Sequential(torch.nn.Linear(layer_sizes[4], layer_sizes[5]), GELU()),\n",
        "    ])\n",
        "  def forward(self, input_embeddings):\n",
        "    for layer in self.layers:\n",
        "      layer_output = layer(input_embeddings)\n",
        "      if self.use_shortcut and input_embeddings.shape == layer_output.shape:\n",
        "        input_embeddings = input_embeddings + layer_output\n",
        "      else:\n",
        "        input_embeddings = layer_output\n",
        "    return input_embeddings\n",
        "\n",
        "\n",
        "layer_size = [3, 3, 3, 3, 3, 1]\n",
        "sample_input = torch.tensor([[1., 0., -1]])\n",
        "torch.manual_seed(123)\n",
        "model_without_shortcut = ExampleDeepNeuralNetwork(layer_size, False)\n",
        "torch.manual_seed(123)\n",
        "model_with_shortcut = ExampleDeepNeuralNetwork(layer_size, True)\n",
        "\n",
        "\n",
        "def print_gradients(model, x):\n",
        "  output = model(x)\n",
        "  target = torch.tensor([[0.]])\n",
        "\n",
        "  loss = torch.nn.MSELoss()\n",
        "  loss = loss(output, target)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  for name, param in model.named_parameters():\n",
        "    if 'weight' in name:\n",
        "      print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\n",
        "\n",
        "print(\"No shortcut connection\")\n",
        "print_gradients(model_without_shortcut, sample_input)\n",
        "\n",
        "print(\"\\nWith shortcut connection\")\n",
        "print_gradients(model_with_shortcut, sample_input)"
      ],
      "metadata": {
        "id": "jBfwPDOUuzJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Text"
      ],
      "metadata": {
        "id": "qJ4RrDz00aKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, input_idx, max_new_tokens, context_size):\n",
        "\n",
        "  # Generate up to max_new_tokens of tokens\n",
        "  for _ in range(max_new_tokens):\n",
        "    # get the last context_size tokens if the input is longer than context_size\n",
        "    idx_cond = input_idx[:, -context_size:]\n",
        "\n",
        "    # Step 1: produce the output logits\n",
        "    with torch.no_grad():\n",
        "      logits = model(idx_cond)\n",
        "\n",
        "    # Step 2: get the last vector of the logits\n",
        "    logits = logits[:, -1, :]\n",
        "\n",
        "    # Step 3: get probability from logits using softmax\n",
        "    prob = torch.softmax(logits, dim=-1)  # (batch_size, vocab_size)\n",
        "\n",
        "    # Step 4: find the position with largest probability\n",
        "    predicted_tokens = torch.argmax(prob, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "    # Step 5: append the predicted token to the previous input tokens\n",
        "    input_idx = torch.cat((input_idx, predicted_tokens), dim=1)  # (batch_size, n_tokens + 1)\n",
        "\n",
        "  return input_idx"
      ],
      "metadata": {
        "id": "3VsQdG690bmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "# Disable dropout since we are not training the model\n",
        "gpt.eval()\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "with open('the-verdict.txt', 'r', encoding='utf-8') as f:\n",
        "  raw_txt = f.read()\n",
        "\n",
        "dataloader = create_dataloader_v1(raw_txt, batch_size=2, max_length=5, stride=10)\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "\n",
        "print('Input:')\n",
        "print(inputs.shape)\n",
        "print(tokenizer.decode(inputs[0].squeeze(0).tolist()))\n",
        "\n",
        "outputs = generate_text(gpt, inputs, 10, GPT_CONFIG_124M['context_length'])\n",
        "\n",
        "print('\\nOutput:')\n",
        "print(outputs.shape)\n",
        "print(tokenizer.decode(outputs[0].squeeze(0).tolist()))"
      ],
      "metadata": {
        "id": "x0BOZXbs20Dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "B3g1jfKYLpgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross Entropy and Perplexity\n",
        "\n",
        "def cross_entropy_loss(logits, targets):\n",
        "  logits_flat = logits.flatten(0, 1)\n",
        "  targets_flat = targets.flatten()\n",
        "  return torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "def cross_entropy_loss_v1(logits, targets):\n",
        "  batch_size, seq_len, vocab_size = logits.shape\n",
        "  probas = torch.softmax(logits, dim=-1)\n",
        "\n",
        "  target_probas = []\n",
        "  for i in range(batch_size):\n",
        "    target_proba = probas[i, [j for j in range(seq_len)], targets[i]]\n",
        "    print(target_proba)\n",
        "    target_probas.append(target_proba)\n",
        "  log_probas = torch.log(torch.cat(target_probas))\n",
        "  avg_log_probas = torch.mean(log_probas)\n",
        "  return -avg_log_probas\n",
        "\n",
        "# Disable dropout since we are not training the model\n",
        "# gpt.eval()\n",
        "\n",
        "# data_iter = iter(dataloader)\n",
        "# inputs, targets = next(data_iter)\n",
        "# print(inputs)\n",
        "# print(targets)\n",
        "\n",
        "# with torch.no_grad():\n",
        "#   logits = gpt(inputs)\n",
        "\n",
        "# loss = cross_entropy_loss(logits, targets)\n",
        "# print(\"loss: \", loss)\n",
        "\n",
        "# perplexity = torch.exp(loss)\n",
        "# print(\"perplexity: \", perplexity)"
      ],
      "metadata": {
        "id": "u3Ex5btfLqyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('the-verdict.txt', 'r', encoding='utf-8') as f:\n",
        "  raw_txt = f.read()\n",
        "\n",
        "print(\"Characters: \", len(raw_txt))\n",
        "print(\"Tokens: \", len(tokenizer.encode(raw_txt)))\n",
        "\n",
        "train_ratio = 0.9\n",
        "split_idx = int(train_ratio * len(raw_txt))\n",
        "train_data = raw_txt[:split_idx]\n",
        "val_data = raw_txt[split_idx:]\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_dataloader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M['context_length'],\n",
        "    stride=GPT_CONFIG_124M['context_length'],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0)\n",
        "\n",
        "val_dataloader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M['context_length'],\n",
        "    stride=GPT_CONFIG_124M['context_length'],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0)\n",
        "\n",
        "# Sanity check\n",
        "print('Train dataloader:')\n",
        "for x, y in train_dataloader:\n",
        "  print(x.shape, y.shape)\n",
        "print('\\nValidation dataloader:')\n",
        "for x, y in val_dataloader:\n",
        "  print(x.shape, y.shape)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "gpt = GPTModel(GPT_CONFIG_124M)\n",
        "\n",
        "def calculate_loss_batch(input_batch, target_batch, model, device):\n",
        "  # 2 x 256\n",
        "  input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "  # 2 x 256 x 50257\n",
        "  logits = model(input_batch)\n",
        "  loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "  return loss\n",
        "\n",
        "def calculate_loss_loader(data_loader, model, device, num_batches=None):\n",
        "  total_loss = 0\n",
        "  if len(data_loader) == 0:\n",
        "    return float('nan')\n",
        "  elif num_batches is None:\n",
        "    num_batches =len(data_loader)\n",
        "  else:\n",
        "    num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "  for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "    if i < num_batches:\n",
        "      loss = calculate_loss_batch(input_batch, target_batch, model, device)\n",
        "      total_loss += loss\n",
        "    else:\n",
        "      break\n",
        "  return total_loss / num_batches\n"
      ],
      "metadata": {
        "id": "3byRrbNlVbNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "elif torch.backends.mps.is_available():\n",
        "  device = torch.device('mps')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "gpt.to(device)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "with torch.no_grad(): # Disable gradient tracking for efficiency bc we're not training yet\n",
        "  train_loss = calculate_loss_loader(train_dataloader, gpt, device)\n",
        "  val_loss = calculate_loss_loader(val_dataloader, gpt, device)\n",
        "print(f\"Training loss: {train_loss}, Validation loss: {val_loss}\")"
      ],
      "metadata": {
        "id": "5-N39AFrcH0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5NL8pS6UjEkM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}